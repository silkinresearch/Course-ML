{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a1f9ee1",
   "metadata": {},
   "source": [
    "Пусть у нас есть $n$ предикторов: <br>\n",
    "$x = (1, x_1, x_2, ..., x_n)$ — вектор-строка <br>\n",
    "$w = (w_0, w_1, w_2, ..., w_n)$ — вектор-строка <br>\n",
    "\n",
    "$$ \\boxed{p_i = \\sigma \\Big(\\sum_{i = 0}^{n} w_i \\cdot x_i \\Big) = \\sigma(\\mathbf{w \\cdot x})}$$\n",
    "\n",
    "Можно показать, что если в качестве функции активации (которая переводит линейный выход в промежуток (0, 1)) взять сигмоиду ($\\sigma(logit) = (1+exp(-logit))^{-1}$, то минимум бинарной кросс-энтропии будет достигаться, когда предсказанные значения совпадут с исходными метками.\n",
    "\n",
    "**В матрицах**\n",
    "\n",
    "$$\\frac{\\partial L(\\mathbf{w})}{\\partial \\mathbf{w}} =  \\frac{\\partial L}{\\partial \\mathbf{p}} \\underbrace{\\frac{\\partial \\mathbf{p}}{\\partial \\mathbf{z}}}_{\\sigma'(z)} \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{w}} = (\\mathbf{p - t}) X$$\n",
    "\n",
    "**Производная по весам**\n",
    "\n",
    "$$\\begin{pmatrix} \\frac{\\partial L}{\\partial \\omega_1}  & \\frac{\\partial L}{\\partial \\omega_2} &  ... & \\frac{\\partial L}{\\partial \\omega_n} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial L}{\\partial p_1} & \\frac{\\partial L}{\\partial p_2} & ... & \\frac{\\partial L}{\\partial p_k} \\end{pmatrix} \\cdot \\begin{pmatrix} \\frac{\\partial p_1}{\\partial z_1}  & \\frac{\\partial p_1}{\\partial z_2} & ... & \\frac{\\partial p_1}{\\partial z_k}\\\\ \\frac{\\partial p_2}{\\partial z_1}  & \\frac{\\partial p_2}{\\partial z_2} & ... & \\frac{\\partial p_2}{\\partial z_k} \\\\ ... & ... & ... & ... \\\\ \\frac{\\partial p_k}{\\partial z_1} & \\frac{\\partial p_k}{\\partial z_2} & ... &\\frac{\\partial p_k}{\\partial z_k} \\end{pmatrix} \\cdot \\begin{pmatrix}  \\frac{\\partial z_1}{\\partial \\omega_1}  & \\frac{\\partial z_1}{\\partial \\omega_2} &  ... & \\frac{\\partial z_1}{\\partial \\omega_n} \\\\  \\frac{\\partial z_2}{\\partial \\omega_1}  & \\frac{\\partial z_2}{\\partial \\omega_2} &  ... & \\frac{\\partial z_2}{\\partial \\omega_n} \\\\  ... & ... & ... \\\\  \\frac{\\partial z_k}{\\partial \\omega_1}  & \\frac{\\partial z_k}{\\partial \\omega_2} &  ... & \\frac{\\partial z_k}{\\partial \\omega_n}\\end{pmatrix} $$\n",
    "\n",
    "$$\\begin{pmatrix} \\frac{\\partial L}{\\partial \\omega_1}  & \\frac{\\partial L}{\\partial \\omega_2} & ... & \\frac{\\partial L}{\\partial \\omega_n} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial L}{\\partial p_1} & \\frac{\\partial L}{\\partial p_2} & ... & \\frac{\\partial L}{\\partial p_k} \\end{pmatrix} \\cdot \\begin{pmatrix} \\frac{\\partial p_1}{\\partial z_1}  & 0 & ... & 0\\\\ 0  & \\frac{\\partial p_2}{\\partial z_2} & ... & 0 \\\\ ... & ... & ... & ... \\\\ 0 & 0 & ... &\\frac{\\partial p_k}{\\partial z_k} \\end{pmatrix} \\cdot \\begin{pmatrix}  x_{11}  & x_{12} &  ... & x_{1n} \\\\  x_{21}  & x_{22} &  ... & x_{2n} \\\\  ... & ... & ... \\\\  x_{k1}  & x_{k2} &  ... & x_{kn}\\end{pmatrix} $$\n",
    "\n",
    "\n",
    "\n",
    "$$\\begin{pmatrix} \\frac{\\partial L}{\\partial \\omega_1}  & \\frac{\\partial L}{\\partial \\omega_2} &  ... & \\frac{\\partial L}{\\partial \\omega_n} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial L}{\\partial p_1} \\cdot \\frac{\\partial p_1}{\\partial z_1 } & \\frac{\\partial L}{\\partial p_2} \\cdot \\frac{\\partial p_2}{\\partial z_2} & ... & \\frac{\\partial L}{\\partial p_k} \\cdot \\frac{\\partial p_k}{\\partial z_k}\\end{pmatrix} \\cdot \\begin{pmatrix}  x_{11}  & x_{12} &  ... & x_{1n} \\\\  x_{21}  & x_{22} &  ... & x_{2n} \\\\  ... & ... & ... \\\\  x_{k1}  & x_{k2} &  ... & x_{kn}\\end{pmatrix} $$\n",
    "\n",
    "**Производная по bias**\n",
    "\n",
    "$$\\begin{pmatrix} \\frac{\\partial L}{\\partial b} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial L}{\\partial p_1} \\cdot \\frac{\\partial p_1}{\\partial z_1 } & \\frac{\\partial L}{\\partial p_2} \\cdot \\frac{\\partial p_2}{\\partial z_2} & ... & \\frac{\\partial L}{\\partial p_k} \\cdot \\frac{\\partial p_k}{\\partial z_k}\\end{pmatrix} \\cdot \\begin{pmatrix}  \\frac{\\partial z_1}{\\partial b} \\\\  \\frac{\\partial z_2}{\\partial b}  \\\\  ...  \\\\  \\frac{\\partial z_k}{\\partial b} \\end{pmatrix} $$\n",
    "\n",
    "\n",
    "\n",
    "Вывод смотреть здесь: \n",
    "1. https://medium.com/codex/chain-rule-differentiation-log-loss-function-d79f223eae5\n",
    "2. https://math.stackexchange.com/questions/2503428/derivative-of-binary-cross-entropy-why-are-my-signs-not-right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef785ea",
   "metadata": {},
   "source": [
    "**На одном примере.**\n",
    "\n",
    "$$BCE(t, p) = - \\Big( t \\cdot log(p) + (1-t) \\cdot log(1-p) \\Big) $$\n",
    "\n",
    "**Производная бинарной кросс-энтропии по параметрам Логистической Регрессии:**\n",
    "\n",
    "$$\\frac{\\partial BCE(t, p)}{\\partial p} = -\\frac{t}{p} + \\frac{1-t}{1-p} = \\frac{p-t}{p(1-p)}$$\n",
    "\n",
    "$$\\frac{\\partial \\sigma(logit)}{\\partial (logit)} = \\frac{\\partial}{\\partial logit} \\Big(\\frac{1}{1+e^{-logit}} \\Big) = \\frac{e^{-logit}}{(1+e^{-logit})^2} = p(1-p)$$\n",
    "\n",
    "$$\\frac{\\partial logit}{ \\partial \\omega} = \\frac{\\partial}{\\partial \\omega} (\\omega \\cdot x + b) = x$$\n",
    "\n",
    "$$\\frac{\\partial logit}{ \\partial b} = \\frac{\\partial}{\\partial b} (\\omega \\cdot x + b) = 1$$\n",
    "\n",
    "**ИТОГО:**\n",
    "\n",
    "$$\\frac{\\partial BCE(t, p)}{\\partial \\omega} = \\frac{\\partial BCE(t, p)}{\\partial p} \\cdot \\frac{\\partial \\sigma(logit)}{\\partial (logit)} \\cdot \\frac{\\partial (logit)}{\\partial \\omega} = x(p - t)$$\n",
    "\n",
    "$$\\frac{\\partial BCE(t, p)}{\\partial b} = \\frac{\\partial BCE(t, p)}{\\partial p} \\cdot \\frac{\\partial \\sigma(logit)}{\\partial (logit)} \\cdot \\frac{\\partial (logit)}{\\partial b} = p - t$$\n",
    "\n",
    "**Минимальное значение бинарной кросс-энтропии**\n",
    "\n",
    "$$BCE_{max}(t, p)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a898d0a8",
   "metadata": {},
   "source": [
    "https://math.stackexchange.com/questions/2503428/derivative-of-binary-cross-entropy-why-are-my-signs-not-right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea8a805",
   "metadata": {},
   "source": [
    "# Почему логистическая функция"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1648381d",
   "metadata": {},
   "source": [
    "Функция активации - функция, которая переводит выходы линейного слоя (логиты) в шкалу (0, 1). \n",
    "\n",
    "Если в качестве функции активации мы будем использовать сигмоиду, то:\n",
    "- выходы линейного слоя для примера будут иметь физический смысл логарифма шансов иметь целевой признак.\n",
    "- (??? Проверить!)выход модели после применения функции активации будет иметь физический смысл вероятности иметь целевой признак.\n",
    "\n",
    "Это утвержедение выводится через теорему Байеса. Хорошо показано в [треде на Quora](https://www.quora.com/What-is-the-intuition-behind-sigmoid-function):\n",
    "\n",
    "$$P(y=1|x) = \\frac{P(x|y=1) \\cdot P(y=1)}{P(x)}$$\n",
    "\n",
    "$$P(y=1|x) = \\frac{P(x|y=1) \\cdot P(y=1)}{P(x|y=1)\\cdot P(y=1) + P(x|y=0) \\cdot P(y=0)}$$\n",
    "\n",
    "$$P(y=1|x) = \\frac{1}{1 + \\frac{P(y=0)}{P(y=1)} \\frac{P(x|y=0)}{P(x|y=1)}}$$\n",
    "\n",
    "$$P(y=1|x) = \\frac{1}{1 + e^{ln \\Big( \\frac{P(y=0)}{P(y=1)} \\frac{P(x|y=0)}{P(x|y=1)}\\Big)}}$$\n",
    "\n",
    "$$\\boxed{P(y=1|x) = \\frac{1}{1 + e^{- ln \\Big( \\frac{P(y=1)}{P(y=0)} \\frac{P(x|y=1)}{P(x|y=0)}\\Big)}}}$$\n",
    "\n",
    "Cледующее выражение называется \"шансами\" иметь целевой признак. Шансы показывают, во сколько раз у объекта больше вероятность иметь признаки x и пренадлежать классу 1, чем вероятность иметь признаки x, и принадлежать классу 0.\n",
    "\n",
    "$$Odds = \\frac{P(x|y=1)}{P(x|y=0)}$$\n",
    "\n",
    "Таким образом, чтобы научиться определять вероятность того, что объект принадлежит к классу 1, нам необходимо на каком-то внешнем датасете, в котором для каждого наблюдения указано, y = 0 или y = 1, обучить нашу модель считать шансы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96d6e17",
   "metadata": {},
   "source": [
    "# Доверительный интервал для коэффициентов логрега"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1355b3ff",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/27928275/find-p-value-significance-in-scikit-learn-linearregression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674b28ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "X2 = sm.add_constant(X)\n",
    "est = sm.OLS(y, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c015b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(X,y)\n",
    "params = np.append(lm.intercept_, lm.coef_)\n",
    "predictions = lm.predict(X)\n",
    "\n",
    "newX = pd.DataFrame({\"Constant\":np.ones(len(X))}).join(pd.DataFrame(X))\n",
    "MSE = (sum((y-predictions)**2))/(len(newX)-len(newX.columns))\n",
    "\n",
    "var_b = MSE*(np.linalg.inv(np.dot(newX.T,newX)).diagonal())\n",
    "sd_b = np.sqrt(var_b)\n",
    "ts_b = params/ sd_b\n",
    "\n",
    "p_values =[2*(1-stats.t.cdf(np.abs(i),(len(newX)-len(newX[0])))) for i in ts_b]\n",
    "\n",
    "sd_b = np.round(sd_b,3)\n",
    "ts_b = np.round(ts_b,3)\n",
    "p_values = np.round(p_values,3)\n",
    "params = np.round(params,4)\n",
    "\n",
    "myDF3 = pd.DataFrame()\n",
    "myDF3[\"Coefficients\"],myDF3[\"Standard Errors\"],myDF3[\"t values\"],myDF3[\"Probabilities\"] = [params,sd_b,ts_b,p_values]\n",
    "print(myDF3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55556074",
   "metadata": {},
   "source": [
    "Вот для логрега\n",
    "https://stackoverflow.com/questions/22306341/python-sklearn-how-to-calculate-p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e52cae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
